{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRcQIRrHVCQRn757lqM+S2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8SSFJOGclos"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ],
      "metadata": {
        "id": "-kqUulwJkl4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "def scrape_profiles(url):\n",
        "    \"\"\"Scrape public profiles from the provided URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch the URL: {url} (Status Code: {response.status_code})\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    profiles = []\n",
        "\n",
        "    for profile in soup.find_all('div', class_='profile-card'):\n",
        "\n",
        "        name = profile.find('h2', class_='name')\n",
        "        job_title = profile.find('p', class_='job-title')\n",
        "        location = profile.find('span', class_='location')\n",
        "        about = profile.find('div', class_='about')\n",
        "\n",
        "        profiles.append({\n",
        "            \"Name\": name.text.strip() if name else \"N/A\",\n",
        "            \"Job Title\": job_title.text.strip() if job_title else \"N/A\",\n",
        "            \"Location\": location.text.strip() if location else \"N/A\",\n",
        "            \"About\": about.text.strip() if about else \"N/A\"\n",
        "        })\n",
        "\n",
        "    return profiles\n",
        "\n",
        "\n",
        "def scrape_multiple_pages(base_url, num_pages=1):\n",
        "    \"\"\"Scrape multiple pages by iterating over the page numbers.\"\"\"\n",
        "    all_profiles = []\n",
        "\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{https://www.linkedin.com/in/shirin-raina-807103174/}/page/{page_num}\"\n",
        "{page_num}\"  # Example URL structure for pagination\n",
        "        print(f\"Scraping page {page_num}...\")\n",
        "        profiles = scrape_profiles(url)\n",
        "        all_profiles.extend(profiles)\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "    return all_profiles\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    \"\"\"Save scraped data to a CSV file.\"\"\"\n",
        "    if data:\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Data saved to {filename}\")\n",
        "    else:\n",
        "        print(\"No data to save.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    base_url = \"https://www.linkedin.com/company/athena-executive-search-&-consulting-aesc-/?originalSubdomain=in\"\n",
        "    profiles_data = scrape_multiple_pages(base_url, num_pages=3)\n",
        "    save_to_csv(profiles_data, \"profiles.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HT1E7hMbd9uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38bvl58eeBxk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}